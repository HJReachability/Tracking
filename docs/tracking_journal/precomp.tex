% !TEX root = tracking.tex
\section{Offline Computation \label{sec:precomp}}
The offline computation begins with setting up a pursuit-evasion game \cite{Huang11, Chen17} between the tracking system and the planning system. 
In this game, the tracking system will try to ``capture" the planning system, while the planning system is doing everything it can to avoid capture. 
In reality the planner is typically not actively trying to avoid the tracking system, but this allows us to account for worst-case scenarios and more crucially, ensure that the TEB is \textit{trajectory-independent}. 
If both systems are acting optimally in this way, we want to determine the largest relative distance (based on some suitable metric) that may occur over time. 
This distance is the maximum possible tracking error between the two systems, and is captured by the value function obtained from solving the HJ VI \eqref{eq:HJVI}.

\subsection{Relative Dynamics}
To determine the relative distance that may occur over time we must first define the relative states and dynamics between the tracking and planning models. The individual dynamics are defined in Section \ref{sec:formulation}, equations (\ref{eq:tdyn}) and (\ref{eq:pdyn}). The relative system is found by fixing the planning model to the origin and finding the dynamics of the tracking model relative to the planning model, as shown below.

\begin{equation}
\label{eq:rdyn}
\begin{aligned}
\rstate = \tstate - \ptmat\pstate, \qquad \dot\rstate = \rdyn(\rstate, \tctrl, \pctrl, \dstb)
\end{aligned}
\end{equation}

\noindent where $\ptmat$ matches the common states of $\tstate$ and $\pstate$ by augmenting the state space of the planning model (as shown in Section \ref{sec:results}).
The relative states $\rstate$ now represent the tracking states relative to the planning states. 
Similarly, $\tpmat$ projects the state space of the tracking model onto the planning model: $\pstate = \tpmat(\tstate-\rstate)$. 
This will be used to update the planning state in the online algorithm.

\subsection{Formalizing the Pursuit-Evasion Game}
Given the relative dynamics between the tracking system and the planning system, we would like to compute a guaranteed TEB between these systems. 
This is done by first defining a error function $\errfunc(\rstate)$ in the relative state space of the systems. 
One typical error function is distance to the origin, which corresponds to the tracking error in terms of Euclidean distance between the tracking and planning systems. 
This error function is shown in Fig. \ref{fig:vf_TEB:5D3D}. 
Of course, the error function can be defined in any desired manner, and involve planning state variables other than position.
For example, the error function, which corresponds to $\valfunc(\rstate, 0)$ in Fig. \ref{fig:vf_TEB:8D4D}, is defined in both position and velocity space; Fig. \ref{fig:vf_TEB_10D3D} shows yet another error function defined using the one-norm of the displacement between the two systems. 
In our pursuit-evasion game formulation, the tracking system tries to minimize this cost, while the planning system and any disturbances experienced by the tracking system try to do the opposite -- maximize.

\begin{figure}
  \includegraphics[width=\columnwidth]{fig/ti_valfunc_5d3d}
  \caption{Infinite time horizon TEB (top left), two slices of the value function at $\theta_r = \pi/2, -3\pi/4$ (top right, bottom left), and corresponding TEB slices (bottom right) for the 5D car tracking 3D Dubins car example in Section \ref{sec:reach_planner}.}
  \label{fig:vf_TEB:5D3D}  
\end{figure}

\begin{figure}
	\centering
  \includegraphics[width=\columnwidth]{fig/tv_valfunc}
  \caption{Finite time horizon value function (top) and TEBs (bottom) for the 8D quadrotor tracking 4D double integrator example in Section \ref{sec:resultsMPC}.}  
  \label{fig:vf_TEB:8D4D}
\end{figure} 




Before constructing the pursuit-evasion game we must first define the method each player must use for making decisions. 
We define a strategy for planning system as the mapping $\gamma_{\pstate} : \tcset \rightarrow \pcset$ that determines a control for the planning model based on the control of the planning model. We restrict $\gamma$ to draw from only non-anticipative strategies $\gamma_{\pstate} \in \Gamma_\pstate(t)$, as defined in \cite{Mitchell05}. 
We similarly define the disturbance strategy $\gamma_{\dstb}: \tcset \rightarrow \dset$, $\gamma_{\dstb} \in \Gamma_\dstb(t)$.

We compute the highest cost that this game will ever attain when both players are acting optimally. 
This is expressed through the following value function:
\begin{align}
&V(\rstate,\thor)= \sup_{\gamma_{\pstate} \in \Gamma_\pstate(t), \gamma_{\dstb} \in \Gamma_\dstb(t)} \inf_{\tctrl(\cdot) \in \tcfset(t)} \big\{ \nonumber \\
&\qquad\qquad \max_{\tvar\in [0, \thor]} \errfunc\Big(\rtraj(\tvar; \rstate, 0, \tctrl(\cdot), \gamma_\pstate[\tctrl](\cdot), \gamma_\dstb[\tctrl](\cdot))\Big)\big\}. \label{eq:valfunc}
\end{align} 

The value function can be computed via existing methods in HJ reachability analysis \cite{}.
Adapting the formulation in \cite{Fisac15}, we compute the value function by solving the following HJ VI:

\begin{align}
\max \Big\{&\frac{\partial \valfunc}{\partial \tvar} + \min_{\tctrl\in\tset} \max_{\pctrl\in\pset, \dstb\in\dset} \deriv \cdot \rdyn(\rstate, \tctrl, \pctrl, \dstb), \nonumber \\
&\qquad\errfunc(\rstate) - \valfunc(\rstate, \tvar)\Big\} = 0, \quad \tvar \in [-T, 0], \label{eq:HJVI} \\
&\valfunc(\rstate, 0) = \errfunc(\rstate). \nonumber
\end{align}

If the control authority of the tracking system is powerful enough to always eventually remain within some distance from the planning system, this value function will converge to an invariant solution for all time, i.e. $\valfunc_\infty(\rstate) := \lim_{\thor\rightarrow\infty} \valfunc(\rstate, \thor)$. 
An example of this converged value function is in Fig. \ref{fig:vf_TEB:5D3D}. 
In some cases, the value function does not converge, and provides a finite time horizon, time-varying TEB, an example of which is shown in Fig. \ref{fig:vf_TEB:8D4D}.
In Section \ref{sec:proofs}, we will formally prove that sublevel sets of $\valfunc(\rstate,\tvar)$ and $\valfunc_\infty(\rstate$ respectively provide the corresponding time-varying TEBs $\TEB(t)$ for the finite time horizon case, and time-invariant $\TEB_\infty$ for the infinite time horizon case.
 
The spatial gradients of the value function, $\deriv(\rstate, \tvar)$ or $\deriv_\infty(\rstate)$, comprise the safety controller function (as described in Section \ref{sec:online}). 
When the framework is executed on a computer, these two functions are saved as look-up tables over a grid representing the state space of the relative system.
 
\subsection{Error Bound Guarantee via Value Function} \label{sec:proofs}
We now state the main theoretical results of this paper in Propositions \ref{prop:nonconv} and \ref{prop:main}, which state that every level set of $\valfunc(\rstate, \tvar)$ in the finite time horizon case and $\valfunc_\infty(\rstate)$ in the infinite time horizon case respectively is invariant under the following conditions:
\begin{enumerate}
  \item The tracking system applies the optimal control which tries to track the planning system;
  \item The planning system applies (at worst) the optimal control that tries to escape from the tracking system; \label{ln:plan}
  \item The tracking system experiences (at worst) the optimal disturbance that tries to prevent successful tracking. \label{ln:dist}
\end{enumerate}
In practice, conditions \ref{ln:plan} and \ref{ln:dist} may not hold; the result of this is only advantageous to the tracking system and will make it ``easier'' to stay within its current level set of $\valfunc(\rstate, \tvar)$ or $\valfunc_\infty(\rstate)$. 
The smallest level set corresponding to the value $\underline\valfunc := \min_{\rstate} \valfunc(\rstate,\thor)$ or $\underline\valfunc_\infty := \min_{\rstate} \valfunc_\infty(\rstate)$ can be interpreted as the smallest possible tracking error of the system. 
The TEB is given by the set $\TEB(\tau) = \{\rstate: \valfunc(\rstate, \thor - \tau) \le \underline\valfunc\}$ in the finite time horizon case, and $\TEB_\infty = \{\rstate: \valfunc_\infty(\rstate) \le \underline\valfunc_\infty\}$ in the infinite time horizon case\footnote{In practice, since $\valfunc$ is obtained numerically, we set, for example, $\TEB_\infty = \{\rstate: \valfunc_\infty(\rstate) \le \underline\valfunc_\infty + \epsilon\}$ for some suitably small $\epsilon>0$}. 
This TEB in the planner's frame of reference is given by
\begin{equation} \label{eq:TEBp}
\begin{aligned}
\TEB_\pstate(\tau) = \{\pstate: \exists s, \valfunc(\tstate-\ptmat\pstate, \thor-\tau) \le \underline\valfunc\} \\
\quad \text{(Finite time horizon)}\\
\TEB_{\pstate, \infty} = \{\pstate: \exists s, \valfunc_\infty(\tstate-\ptmat\pstate) \le \underline\valfunc_\infty\} \\
\quad \text{(Infinite time horizon)}\\
\end{aligned}
\end{equation}

This is the TEB that will be used in the online framework as shown in Fig. \ref{fig:fw_online}. 
Within this bound the tracking system may use any controller, but on the boundary\footnote{Practical issues arising from sampled data control can be handled using methods such as \cite{Mitchell2012, Mitchell13, Dabadie2014} and are not the focus of our paper.} of this bound the tracking system must use the safety optimal controller.
In general, the TEB is defined as a set in the state space of the planning system, which allows the TEB to not only be in terms of position, but any state of the planning system such as velocity, as demonstrated in the example in Section \ref{sec:resultsMPC}.

We now formally state and prove the propositions. Note that an interpretation of \eqref{eq:TEBp} is that $\walfunc(\rstate, t) := \valfunc(\rstate, \thor - \tvar)$ and $\valfunc_\infty(\rstate)$ are control-Lyapunov functions for the relative dynamics between the tracking system and the planning system.

\begin{prop}
  \label{prop:nonconv}
  \textbf{Finite time horizon guaranteed TEB.}
  Given $\tvar, \tvar' \in [0, \thor]$,
  
  \begin{subequations} \label{eq:fin_thor_prop}
      \begin{align}
      \forall \tvar' \ge \tvar, &~ \rstate \in \TEB(\tvar) \Rightarrow \rtraj^*(\tvar'; \rstate, \tvar) \in \TEB(\tvar'), \\
      \text{where} &\quad \rtraj^*(\tvar; \rstate, 0) := \rtraj(\tvar; \rstate, 0, \tctrl^*(\cdot), \pctrl^*(\cdot), \dstb^*(\cdot))), \label{eq:fin_thor_prop:here} \\
      &\quad
      \begin{aligned}
      &\tctrl^*(\cdot) = \arg \inf_{\tctrl(\cdot)\in\tcfset(t)}\big\{\\
      & \quad \max_{\tvar \in [0, \thor]} \errfunc(\rtraj(\tvar; \rstate, 0, \tctrl(\cdot), \pctrl^*(\cdot), \dstb^*(\cdot))) \big\},\\
      \end{aligned} \\
      &\quad
      \begin{aligned}
      & \pctrl^*(\cdot) := \gamma_\pstate^*[\tctrl](\cdot) = \arg \sup_{\gamma_{\pstate} \in \Gamma_\pstate(t)} \inf_{\tctrl(\cdot) \in \tcfset(t)} \big\{ \\
      & \quad \max_{t \in [0, \thor]} \errfunc(\rtraj(\tvar; \rstate, 0, \tctrl(\cdot), \gamma_\pstate[\tctrl](\cdot), \dstb^*(\cdot))) \big\} \\
      \end{aligned} \\
      &\quad
      \begin{aligned}
      & \dstb^*(\cdot) = \arg \sup_{\gamma_{\dstb} \in \Gamma_\dstb(t)} \sup_{\gamma_{\pstate} \in \Gamma_\pstate(t)} \inf_{\tctrl(\cdot) \in \tcfset(t)} \big\{\\
      & \quad \max_{\tvar \in [0, \thor]} \errfunc(\rtraj(\tvar; \rstate, 0, \tctrl(\cdot), \gamma_\pstate[\tctrl](\cdot), \gamma_\dstb[\tctrl](\cdot))) \big\}
      \end{aligned} \label{eq:fin_thor_prop:there}
      \end{align}
  \end{subequations}

\end{prop}

\begin{IEEEproof}
  
  We first show that given $\tvar, \tvar' \in [0, \thor]$,
  
  \begin{equation} \label{eq:vf_nondec}
    \forall \tvar' \ge \tvar, ~\valfunc(\rstate, \thor - \tvar) \ge \valfunc(\rtraj^*(\tvar'; \rstate, \tvar), \thor - \tvar')
  \end{equation}
  
  This follows from the definition of value function.
  \begin{subequations} \label{eq:fin_thor_steps}
    \begin{align}
      \valfunc(\rstate, \thor - \tvar) & = \max_{\tau \in [0, \thor-\tvar]} \errfunc(\rtraj^*(\tau; \rstate, 0)) \label{eq:fin_thor_steps:1} \\
      & = \max\{ \max_{\tau \in [0, \tvar'-\tvar]} \errfunc(\rtraj^*(\tau; \rstate, 0)), \nonumber\\
      &\qquad\qquad \max_{\tau \in [\tvar'-\tvar, \thor-\tvar]} \errfunc(\rtraj^*(\tau; \rstate, 0)) \} \label{eq:fin_thor_steps:2}\\
      & \ge \max_{\tau \in [\tvar'-\tvar, \thor-\tvar]} \errfunc(\rtraj^*(\tau; \rstate, 0)) \label{eq:fin_thor_steps:3}\\
      & = \max_{\tau \in [0, \thor-\tvar']} \errfunc(\rtraj^*(\tau; \rstate, \tvar-\tvar')) \label{eq:fin_thor_steps:4}\\
      & = \max_{\tau \in [0, \thor-\tvar']} \errfunc(\rtraj^*(\tau; \rtraj^*(0; \rstate, \tvar-\tvar'), 0)) \label{eq:fin_thor_steps:5}\\  
      & = \max_{\tau \in [0, \thor-\tvar']} \errfunc(\rtraj^*(\tau; \rtraj^*(\tvar'; \rstate, \tvar), 0)) \label{eq:fin_thor_steps:6}\\      
      & = \valfunc(\rtraj^*(\tvar'; \rstate, \tvar), \thor - \tvar') \label{eq:fin_thor_steps:7}
    \end{align}
  \end{subequations}

Explanation of steps:
\begin{itemize}
  \item \eqref{eq:fin_thor_steps:1} and \eqref{eq:fin_thor_steps:7}: by definition of value function
  \item \eqref{eq:fin_thor_steps:2}: rewriting $\max_{\tau \in [0, \thor-\tvar]}$ by splitting up the time interval $[0, \thor-\tvar]$ into $[0, \tvar'-\tvar]$ and $[\tvar'-\tvar, \thor-\tvar]$
  \item \eqref{eq:fin_thor_steps:3}: ignoring first argument of the outside $\max$ operator
  \item \eqref{eq:fin_thor_steps:4}: shifting time reference by $\tvar-\tvar'$, since dynamics are time-invariant
  \item \eqref{eq:fin_thor_steps:5}: splitting trajectory $\rtraj^*(\tau; \rstate, \tvar-\tvar')$ into two stages corresponding to time intervals $[\tvar-\tvar', 0]$ and $[0, \tau]$
  \item \eqref{eq:fin_thor_steps:6}: shifting time reference in $\rtraj^*(0; \rstate, \tvar-\tvar')$ by $\tvar'$, since dynamics are time-invariant
\end{itemize}

Now, we finish the proof as follows:
\begin{subequations} \label{eq:fin_hor}
  \begin{align}
  \rstate \in \TEB(\tvar) &\Leftrightarrow \valfunc(\rstate, \thor - \tvar) \le \underline\valfunc \\
  & \Rightarrow  \valfunc(\rtraj^*(\tvar'; \rstate, \tvar), \thor - \tvar') \le \underline\valfunc \label{eq:fin_hor:2}\\
  & \Leftrightarrow \rtraj^*(\tvar'; \rstate, \tvar) \in \TEB(\tvar'),
  \end{align}
\end{subequations}

\noindent where $\eqref{eq:vf_nondec}$ is used for the step in \eqref{eq:fin_hor:2}.

\end{IEEEproof} 

 \begin{prop}
   \label{prop:main}
   \textbf{Infinite time horizon guaranteed TEB}. Given $\tvar \ge 0$,
   
   \begin{equation}
     \forall \tvar' \ge \tvar, ~\rstate\in\TEB_\infty \Rightarrow \rtraj^*(\tvar'; \rstate, \tvar) \in \TEB_\infty,
   \end{equation}
   
   \noindent with $\rtraj^*$ defined the same way as in \eqref{eq:fin_thor_prop:here} to \eqref{eq:fin_thor_prop:there}.
   
 \end{prop}


\begin{IEEEproof}
  
  Suppose that the value function converges, and define
  \begin{equation}
  \label{eq:conv_valfunc}
  \valfunc_\infty(\rstate) := \lim_{\thor\rightarrow\infty}\valfunc(\rstate, T)
  \end{equation}
  
  We first show that for all $\tvar, \tvar'$ with $\tvar' \ge \tvar$,
  \begin{equation}
  \label{eq:invariant}
  \valfunc_\infty(\rstate) \ge \valfunc_\infty(\rtraj^*(\tvar'; \rstate, \tvar)).
  \end{equation}
  
Without loss of generality, assume $\tvar=0$. By definition, we have

\begin{subequations} \label{eq:inf_thor_steps}
  \begin{align}
  \valfunc_\infty(\rstate) & = \lim_{\thor\rightarrow\infty}\max_{\tau \in [0, \thor]} \errfunc(\rtraj^*(\tau; \rstate, 0)) \label{eq:inf_thor_steps:1}\\
  &= \lim_{\thor\rightarrow\infty}\max_{\tau \in [-\tvar', \thor]} \errfunc(\rtraj^*(\tau; \rstate, -\tvar')) \label{eq:inf_thor_steps:2}\\
  &\ge \lim_{\thor\rightarrow\infty}\max_{\tau \in [0, \thor]} \errfunc(\rtraj^*(\tau; \rstate, -\tvar')) \label{eq:inf_thor_steps:3}\\
  & = \lim_{\thor\rightarrow\infty}\max_{\tau \in [0, \thor]} \errfunc(\rtraj^*(\tau; \rtraj^*(0; \rstate, -\tvar'), 0)) \label{eq:inf_thor_steps:4}\\
  & = \lim_{\thor\rightarrow\infty}\max_{\tau \in [0, \thor]} \errfunc(\rtraj^*(\tau; \rtraj^*(\tvar'; \rstate, 0), 0)) \label{eq:inf_thor_steps:5}\\
  & = \valfunc_\infty(\rtraj^*(\tvar'; \rstate, 0)) \label{eq:inf_thor_steps:6}
  \end{align}
\end{subequations}

Explanation of steps:
\begin{itemize}
  \item \eqref{eq:inf_thor_steps:1} and \eqref{eq:fin_thor_steps:6}: by definition of value function
  \item \eqref{eq:inf_thor_steps:2}: shifting time by $-\tvar'$
  \item \eqref{eq:inf_thor_steps:3}: removing the time interval $[-\tvar',0)$ in the $\max$ operator
  \item \eqref{eq:inf_thor_steps:4}: splitting trajectory $\rtraj^*(\tau; \rstate, -\tvar')$ into two stages corresponding to time intervals $[-\tvar', 0]$ and $[0, \tau]$
  \item \eqref{eq:inf_thor_steps:5}: shifting time reference in $\rtraj^*(0; \rstate, -\tvar')$ by $\tvar'$, since dynamics are time-invariant
\end{itemize}

Now, we finish the proof as follows:
\begin{subequations} \label{eq:inf_hor}
  \begin{align}
  \rstate \in \TEB_\infty &\Leftrightarrow \valfunc_\infty(\rstate) \le \underline\valfunc \\
  & \Rightarrow \valfunc_\infty(\rtraj^*(\tvar'; \rstate, \tvar)) \le \underline\valfunc \label{eq:inf_hor:2}\\
  & \Leftrightarrow \rtraj^*(\tvar'; \rstate, \tvar) \in \TEB_\infty,
  \end{align}
\end{subequations}

\noindent where $\eqref{eq:invariant}$ is used for the step in \eqref{eq:inf_hor:2}.

\end{IEEEproof} 
 \begin{rem} 
   Propositions \ref{prop:nonconv} and \ref{prop:main} are very similar to well-known results in differential game theory with a slightly different cost function \cite{Akametalu2014}, and has been utilized in the context of using the subzero level set of $\valfunc$ or $\valfunc_\infty$ as a backward reachable set for tasks such as collision avoidance or reach-avoid games \cite{Mitchell05}. In this work we do not assign special meaning to any particular level set, and instead consider all level sets at the same time. This effectively allows us to effectively solve many simultaneous reachability problems in a single computation, thereby removing the need to check whether resulting invariant sets are empty, as was done in \cite{Bansal2017}.
 \end{rem}